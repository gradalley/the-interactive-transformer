{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import (\n",
    "\t\"encoding/json\"\n",
    "\t\"errors\"\n",
    "\t\"io/ioutil\"\n",
    "\t\"strings\"\n",
    "\t\"unicode/utf8\"\n",
    ")\n",
    "\n",
    "const GPT2_EOT int32 = 50256\n",
    "\n",
    "type Tokenizer struct {\n",
    "\tEncoder     map[string]int32 `json:\"encoder\"`\n",
    "\tBpeRanks    map[string]int   `json:\"bpe_ranks\"`\n",
    "\tSpecialTokens map[string]int32 `json:\"special_tokens\"`\n",
    "\tdecoder     map[int32]string\n",
    "}\n",
    "\n",
    "func NewTokenizer(filename string) (*Tokenizer, error) {\n",
    "\tdata, err := ioutil.ReadFile(filename)\n",
    "\tif err != nil {\n",
    "\t\treturn nil, err\n",
    "\t}\n",
    "\n",
    "\tvar t Tokenizer\n",
    "\terr = json.Unmarshal(data, &t)\n",
    "\tif err != nil {\n",
    "\t\treturn nil, err\n",
    "\t}\n",
    "\n",
    "\tt.decoder = make(map[int32]string)\n",
    "\tfor token, id := range t.Encoder {\n",
    "\t\tt.decoder[id] = token\n",
    "\t}\n",
    "\n",
    "\treturn &t, nil\n",
    "}\n",
    "\n",
    "func (t *Tokenizer) Encode(text string) ([]int32, error) {\n",
    "\tif t.Encoder == nil {\n",
    "\t\treturn nil, errors.New(\"tokenizer not initialized\")\n",
    "\t}\n",
    "\n",
    "\tvar tokens []int32\n",
    "\tfor len(text) > 0 {\n",
    "\t\ti := len(text)\n",
    "\t\tfor i > 0 && !utf8.ValidString(text[:i]) {\n",
    "\t\t\ti--\n",
    "\t\t}\n",
    "\t\tif i == 0 {\n",
    "\t\t\treturn nil, errors.New(\"invalid utf-8 string\")\n",
    "\t\t}\n",
    "\t\ttoken := text[:i]\n",
    "\t\ttext = text[i:]\n",
    "\n",
    "\t\tif id, ok := t.Encoder[token]; ok {\n",
    "\t\t\ttokens = append(tokens, id)\n",
    "\t\t} else {\n",
    "\t\t\tbpeToken := t.bpe(token)\n",
    "\t\t\tfor _, bpeSubToken := range strings.Split(bpeToken, \" \") {\n",
    "\t\t\t\tif id, ok := t.Encoder[bpeSubToken]; ok {\n",
    "\t\t\t\t\ttokens = append(tokens, id)\n",
    "\t\t\t\t} else {\n",
    "\t\t\t\t\treturn nil, errors.New(\"unknown token: \" + bpeSubToken)\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\treturn tokens, nil\n",
    "}\n",
    "\n",
    "func (t *Tokenizer) Decode(tokens []int32) (string, error) {\n",
    "\tif t.decoder == nil {\n",
    "\t\treturn \"\", errors.New(\"tokenizer not initialized\")\n",
    "\t}\n",
    "\n",
    "\tvar text strings.Builder\n",
    "\tfor _, token := range tokens {\n",
    "\t\tif token == GPT2_EOT {\n",
    "\t\t\tcontinue\n",
    "\t\t}\n",
    "\t\tif str, ok := t.decoder[token]; ok {\n",
    "\t\t\ttext.WriteString(str)\n",
    "\t\t} else {\n",
    "\t\t\treturn \"\", errors.New(\"unknown token ID\")\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\treturn text.String(), nil\n",
    "}\n",
    "\n",
    "func (t *Tokenizer) bpe(token string) string {\n",
    "\tpairs := getPairs(token)\n",
    "\tif len(pairs) == 0 {\n",
    "\t\treturn token\n",
    "\t}\n",
    "\n",
    "\tfor {\n",
    "\t\tminPair := \"\"\n",
    "\t\tminRank := int(^uint(0) >> 1) // Max int\n",
    "\n",
    "\t\tfor _, pair := range pairs {\n",
    "\t\t\tif rank, ok := t.BpeRanks[pair]; ok {\n",
    "\t\t\t\tif rank < minRank {\n",
    "\t\t\t\t\tminPair = pair\n",
    "\t\t\t\t\tminRank = rank\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\n",
    "\t\tif minPair == \"\" {\n",
    "\t\t\tbreak\n",
    "\t\t}\n",
    "\n",
    "\t\tparts := strings.Split(minPair, \",\")\n",
    "\t\tif len(parts) != 2 {\n",
    "\t\t\tbreak\n",
    "\t\t}\n",
    "\t\tfirst, second := parts[0], parts[1]\n",
    "\t\tnewToken := strings.ReplaceAll(token, first+second, first+\"\\u0000\"+second)\n",
    "\t\ttoken = strings.ReplaceAll(newToken, \"\\u0000\", \"\")\n",
    "\n",
    "\t\tif !strings.Contains(token, \" \") {\n",
    "\t\t\tbreak\n",
    "\t\t}\n",
    "\n",
    "\t\tpairs = getPairs(token)\n",
    "\t}\n",
    "\n",
    "\treturn token\n",
    "}\n",
    "\n",
    "func getPairs(word string) []string {\n",
    "\tpairs := []string{}\n",
    "\tchars := strings.Split(word, \"\")\n",
    "\tfor i := 0; i < len(chars)-1; i++ {\n",
    "\t\tpairs = append(pairs, chars[i]+\",\"+chars[i+1])\n",
    "\t}\n",
    "\treturn pairs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded:  []\n",
      "decoded:  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tokenize some text:  hwllo\n"
     ]
    }
   ],
   "source": [
    "func main(){\n",
    "tokenizer, err := NewTokenizer(\"./tokenizer.json\")\n",
    "if err != nil {\n",
    "    panic(err)\n",
    "}\n",
    "gonbui.RequestInput(\"Tokenize some text: \", false)\n",
    "// reader := bufio.NewReader(os.Stdin)\n",
    "// _, err := reader.ReadString('\\n')\n",
    "// if err != nil {\n",
    "//     panic(err)\n",
    "// }\n",
    "if err != nil { panic(err) }\n",
    "encoded, err := tokenizer.Encode(\"hello there\")\n",
    "fmt.Println(\"encoded: \", encoded)\n",
    "decoded, err := tokenizer.Decode(encoded)\n",
    "fmt.Println(\"decoded: \", decoded)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Go (gonb)",
   "language": "go",
   "name": "gonb"
  },
  "language_info": {
   "codemirror_mode": "",
   "file_extension": ".go",
   "mimetype": "",
   "name": "go",
   "nbconvert_exporter": "",
   "pygments_lexer": "",
   "version": "go1.22.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
